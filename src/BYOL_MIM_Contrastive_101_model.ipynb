{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f90ddd7e-66c5-4483-856b-a1feef30ed23",
      "metadata": {
        "id": "f90ddd7e-66c5-4483-856b-a1feef30ed23",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "#SSL model: BYOL+ MIM + Contrastive Learning\n",
        "#loss = 1 * byol_loss + 0.6 * mim_loss + 0.6 * contrastive_loss\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from byol_pytorch import BYOL\n",
        "from torch.utils.data import DataLoader\n",
        "import random\n",
        "import numpy as np\n",
        "import kornia.augmentation as K\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.datasets import DatasetFolder\n",
        "from torchvision.datasets.folder import default_loader\n",
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "from tqdm import tqdm  # Import tqdm for progress bars\n",
        "\n",
        "# Ensure Reproducibility\n",
        "torch.manual_seed(42)\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "\n",
        "class PlantDocDataset(Dataset):\n",
        "    def __init__(self, folder, target_size=(384,384)):\n",
        "        # We won't do heavy transforms here. We'll let Kornia handle it on GPU.\n",
        "        valid_exts = {'.png', '.jpg', '.jpeg', '.bmp', '.gif'}\n",
        "        files = os.listdir(folder)\n",
        "\n",
        "        self.image_paths = []\n",
        "        for f in files:\n",
        "            _, ext = os.path.splitext(f)\n",
        "            if ext.lower() in valid_exts:\n",
        "                self.image_paths.append(os.path.join(folder, f))\n",
        "\n",
        "        self.target_size = target_size\n",
        "        print(f\"Found {len(self.image_paths)} images in '{folder}'.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.image_paths[idx]\n",
        "        try:\n",
        "            # Open image as PIL, convert to RGB, then resize to target size and convert to Tensor\n",
        "            image = Image.open(path).convert('RGB')\n",
        "            image = transforms.Resize(self.target_size)(image)  # Resize all images to fixed size\n",
        "            image = transforms.ToTensor()(image)  # shape [C,H,W] on CPU\n",
        "            return image\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Skipping corrupted image {path} - {e}\")\n",
        "            # Fallback: move to the next image in a circular manner\n",
        "            return self.__getitem__((idx + 1) % len(self.image_paths))\n",
        "\n",
        "plant_dataset_folder = \"../PlantDoc-Dataset/unlabel\"  # set your path\n",
        "dataset = PlantDocDataset(folder=plant_dataset_folder, target_size=(384,384))\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "train_loader_ssl = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=4,\n",
        "    drop_last=True,\n",
        "    persistent_workers=True\n",
        ")\n",
        "\n",
        "print(\"Data Loaders Ready for SSL!\")\n",
        "\n",
        "\n",
        "class GPUAugment(nn.Module):\n",
        "    def __init__(self, image_size=384):\n",
        "        super().__init__()\n",
        "        self.augs = nn.Sequential(\n",
        "            K.RandomResizedCrop((image_size, image_size), scale=(0.6, 1.0)),  # Increase diversity\n",
        "            K.RandomHorizontalFlip(p=0.5),\n",
        "            K.RandomRotation(degrees=20.0),\n",
        "            K.RandomSolarize(thresholds=0.5, p=0.2),  # NEW: Randomly darken images\n",
        "            K.RandomGrayscale(p=0.2),  # NEW: Removes color, forces shape-based learning\n",
        "            K.ColorJitter(0.4, 0.4, 0.4, 0.1, p=0.8),\n",
        "            K.RandomGaussianBlur((3,3), sigma=(0.1,2.0), p=0.3)\n",
        "        )\n",
        "        # Separate normalization step\n",
        "        self.normalize = K.Normalize(\n",
        "            mean=torch.tensor([0.485, 0.456, 0.406]),\n",
        "            std=torch.tensor([0.229, 0.224, 0.225])\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.augs(x)               # apply random augmentations\n",
        "        x = self.normalize(x)          # then normalize\n",
        "        return x\n",
        "\n",
        "class MLPClassifier(nn.Module):\n",
        "    def __init__(self, in_features, num_classes):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(in_features, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.classifier(x)\n",
        "\n",
        "# Initialize ResNet101 backbone and wrap it in BYOL\n",
        "from torchvision.models import resnet101, ResNet101_Weights\n",
        "resnet = resnet101(weights=ResNet101_Weights.IMAGENET1K_V1)\n",
        "\n",
        "resnet.fc = MLPClassifier(in_features=2048, num_classes=27)\n",
        "\n",
        "\n",
        "# Create a GPU-based augmentation function\n",
        "gpu_augment = GPUAugment(image_size=384).to(device)\n",
        "\n",
        "def batch_augment_fn(batch):\n",
        "    \"\"\"\n",
        "    Expects a (B,C,H,W) Tensor on GPU, returns augmented + normalized Tensor on GPU.\n",
        "    \"\"\"\n",
        "    return gpu_augment(batch)\n",
        "\n",
        "learner = BYOL(\n",
        "    net=resnet,\n",
        "    image_size=384,  # Not used much if we do GPU transforms\n",
        "    hidden_layer='avgpool',\n",
        "    augment_fn=batch_augment_fn,\n",
        "    augment_fn2=batch_augment_fn,\n",
        "    projection_hidden_size = 2048,\n",
        "    moving_average_decay=0.99,\n",
        "    use_momentum=True\n",
        ").to(device)\n",
        "\n",
        "\n",
        "##################################\n",
        "# Define Loss Functions\n",
        "##################################\n",
        "# Masked Image Modeling (MIM)\n",
        "import kornia.losses\n",
        "class MIMLoss(nn.Module):\n",
        "    def __init__(self, min_mask=0.3, max_mask=0.6, alpha=0.5):\n",
        "        super().__init__()\n",
        "        self.min_mask = min_mask\n",
        "        self.max_mask = max_mask\n",
        "        self.alpha = alpha\n",
        "        self.mse_loss = nn.MSELoss()\n",
        "        self.ssim_loss = kornia.losses.SSIMLoss(window_size=11)\n",
        "\n",
        "    def forward(self, x, target):\n",
        "        batch_size, channels, height, width = x.shape\n",
        "        mask_ratio = random.uniform(self.min_mask, self.max_mask)\n",
        "        mask = torch.rand(batch_size, height, width, device=x.device) < mask_ratio\n",
        "        mask = mask.unsqueeze(1).expand(-1, channels, -1, -1)\n",
        "\n",
        "        x_masked = x.clone()\n",
        "        x_masked[mask] = 0\n",
        "\n",
        "        # Normalize after masking\n",
        "        normalize = K.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        x_masked = normalize(x_masked)\n",
        "        target = normalize(target)\n",
        "\n",
        "        mse = self.mse_loss(x_masked, target)\n",
        "        ssim = self.ssim_loss(x_masked, target)\n",
        "\n",
        "        return self.alpha * mse + (1 - self.alpha) * (1 - ssim)\n",
        "\n",
        "\n",
        "# Contrastive Learning Loss (InfoNCE)\n",
        "class ContrastiveLoss(nn.Module):\n",
        "    def __init__(self, temperature):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def forward(self, z_i, z_j):\n",
        "        batch_size = z_i.shape[0]\n",
        "        z_i = F.normalize(z_i, dim=1)\n",
        "        z_j = F.normalize(z_j, dim=1)\n",
        "\n",
        "        logits = torch.mm(z_i, z_j.T) / self.temperature\n",
        "        labels = torch.arange(batch_size).to(z_i.device)\n",
        "\n",
        "        loss = F.cross_entropy(logits, labels)\n",
        "        return loss\n",
        "\n",
        "mim_loss_fn = MIMLoss(min_mask=0.3, max_mask=0.6).to(device)\n",
        "\n",
        "\n",
        "contrastive_loss_fn = ContrastiveLoss(temperature=0.1).to(device)\n",
        "\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    learner.parameters(),\n",
        "    lr=1e-6,  # Start with a very low LR for warmup\n",
        "    weight_decay=1e-5\n",
        ")\n",
        "\n",
        "max_lr = 1e-4\n",
        "min_lr = 1e-6\n",
        "\n",
        "def warmup_lr_scheduler(optimizer, warmup_epochs=10, min_lr=1e-6, max_lr=1e-4):\n",
        "    \"\"\"\n",
        "    Warm-up scheduler: Over 'warmup_epochs' epochs, LR will go from `min_lr` to `max_lr` linearly.\n",
        "    Then you can switch to a different scheduler afterwards.\n",
        "    \"\"\"\n",
        "    # We assume you set the optimizer's \"base LR\" to max_lr ahead of time.\n",
        "    # The factor starts from (min_lr / max_lr) and goes to 1.0.\n",
        "    start_factor = min_lr / max_lr\n",
        "\n",
        "    def lr_lambda(epoch):\n",
        "      progress = epoch / warmup_epochs  # This gives 0 when epoch=0 and 1.0 when epoch==warmup_epochs\n",
        "      factor = start_factor + (1.0 - start_factor) * min(progress, 1.0)\n",
        "      return factor\n",
        "\n",
        "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "\n",
        "optimizer = torch.optim.AdamW(learner.parameters(), lr=max_lr, weight_decay=1e-5)\n",
        "\n",
        "# Create warmup scheduler\n",
        "warmup_epochs = 10\n",
        "warmup_scheduler = warmup_lr_scheduler(\n",
        "    optimizer,\n",
        "    warmup_epochs=warmup_epochs,\n",
        "    min_lr=min_lr,\n",
        "    max_lr=max_lr\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# Gradient Scaler for Mixed Precision\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "# Checkpoint file path\n",
        "checkpoint_path = \"checkpoint.pth\"\n",
        "\n",
        "# Resume training if a checkpoint exists\n",
        "start_epoch = 0\n",
        "best_loss = float('inf')\n",
        "if os.path.exists(checkpoint_path):\n",
        "    print(\"Resuming training from checkpoint:\", checkpoint_path)\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    learner.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "    best_loss = checkpoint['best_loss']\n",
        "    avg_loss = checkpoint['avg_loss']  # Updated here\n",
        "    print(f\"Resumed at epoch {start_epoch} with avg_loss {avg_loss:.4f}\")\n",
        "\n",
        "# Training Loop\n",
        "epochs = 90  # Train\n",
        "\n",
        "for epoch in range(start_epoch, epochs):\n",
        "    learner.train()\n",
        "    total_loss = 0\n",
        "    total_mim_loss = 0\n",
        "    total_contrastive_loss = 0\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Wrap the training data loader with tqdm for a progress bar\n",
        "    pbar = tqdm(enumerate(train_loader_ssl), total=len(train_loader_ssl), desc=f\"Epoch [{epoch+1}/{epochs}]\")\n",
        "    for batch_idx, images in pbar:\n",
        "        images = images.to(device)\n",
        "\n",
        "        with torch.cuda.amp.autocast(dtype=torch.float32):  # Force ResNet to use float32\n",
        "            # Forward Pass (BYOL)\n",
        "            byol_loss = learner(images)\n",
        "            # Forward Pass (MIM)\n",
        "            mim_loss = mim_loss_fn(images, images)  # Target is the original image\n",
        "\n",
        "            # Generate two augmented views for contrastive learning\n",
        "            augmented_one = batch_augment_fn(images)\n",
        "            augmented_two = batch_augment_fn(images)\n",
        "            z_i, _ = learner.online_encoder(augmented_one, return_projection=True)\n",
        "            z_j, _ = learner.online_encoder(augmented_two, return_projection=True)\n",
        "\n",
        "            contrastive_loss = contrastive_loss_fn(z_i, z_j)\n",
        "            # Total Loss (Weighted Sum)\n",
        "            loss = 1 * byol_loss + 0.6 * mim_loss + 0.6 * contrastive_loss\n",
        "\n",
        "\n",
        "        # Backpropagation (with Gradient Accumulation every 2 steps)\n",
        "        scaler.scale(loss).backward()\n",
        "        if (batch_idx + 1) % 2 == 0:\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_mim_loss += mim_loss.item()\n",
        "        total_contrastive_loss += contrastive_loss.item()\n",
        "\n",
        "        # Update tqdm progress bar with current loss values\n",
        "        pbar.set_postfix({\n",
        "            'loss': f\"{loss.item():.4f}\",\n",
        "            'BYOL': f\"{byol_loss.item():.4f}\",\n",
        "            'MIM': f\"{mim_loss.item():.4f}\",\n",
        "            'Contrastive': f\"{contrastive_loss.item():.4f}\"\n",
        "        })\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader_ssl)\n",
        "    avg_mim_loss = total_mim_loss / len(train_loader_ssl)\n",
        "    avg_contrastive_loss = total_contrastive_loss / len(train_loader_ssl)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}] - Total Loss: {avg_loss:.4f} | MIM: {avg_mim_loss:.4f} | Contrastive: {avg_contrastive_loss:.4f}\")\n",
        "\n",
        "    # Update learning rate with warmup for first 10 epochs\n",
        "    if epoch < 10:\n",
        "        warmup_scheduler.step()\n",
        "        print(f\"Warmup Scheduler Updated LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "\n",
        "    # Save the best model if current loss is lower\n",
        "    if avg_loss < best_loss:\n",
        "        best_loss = avg_loss\n",
        "        torch.save(resnet.state_dict(), \"byol_mim_contrastive_best.pth\")\n",
        "        print(f\"Saved Best Model at epoch {epoch+1}!\")\n",
        "\n",
        "    # Save additional .pth and checkpoints every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        torch.save(resnet.state_dict(), f\"byol_mim_contrastive_epoch{epoch+1}.pth\")\n",
        "        checkpoint_name = f\"checkpoint_epoch{epoch+1}.pth\"\n",
        "        checkpoint = {\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': learner.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scaler_state_dict': scaler.state_dict(),\n",
        "            'best_loss': best_loss,\n",
        "            'avg_loss': avg_loss,\n",
        "\n",
        "        }\n",
        "        torch.save(checkpoint, checkpoint_name)\n",
        "\n",
        "print(\"Self-Supervised Pretraining Complete!\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}